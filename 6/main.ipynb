{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Perceptronul și rețele de perceptroni"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ec11df372c4ad82"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T11:10:32.125425200Z",
     "start_time": "2024-10-24T11:10:31.981365200Z"
    }
   },
   "id": "6202512387520fd4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def compute_y(x, W, bias):\n",
    "    return (-x * W[0] - bias) / (W[1] + 1e-10)\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X, y , W, b, current_x, current_y):\n",
    "    x1 = -0.5\n",
    "    y1 = compute_y(x1, W, b)\n",
    "    x2 = 0.5\n",
    "    y2 = compute_y(x2, W, b)\n",
    "    plt.clf()\n",
    "    color = 'r'\n",
    "    if current_y == -1 :\n",
    "        color = 'b'\n",
    "    plt.ylim((-1, 2))\n",
    "    plt.xlim((-1, 2))\n",
    "    plt.plot(X[y == -1, 0], X[y == -1, 1], 'b+')\n",
    "    plt.plot(X[y == 1, 0], X[y == 1, 1], 'r+')\n",
    "    plt.plot(current_x[0], current_x[1], color+'s')\n",
    "    plt.plot([x1, x2] ,[y1, y2], 'black')\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T11:06:03.145042100Z",
     "start_time": "2024-10-24T11:06:03.117036100Z"
    }
   },
   "id": "dbc848bb82eadbc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptroni cu algoritmul Widrow-Hoff pentru OR și XOR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31d499fc6f1d7a31"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # datele OR\n",
    "y = np.array([-1, 1, 1, 1]) # etichetele\n",
    "W = np.zeros(X.shape[1]) # ponderi\n",
    "bias = 0\n",
    "\n",
    "num_epochs = 15\n",
    "learning_rate = 0.1\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    for sample_idx, sample in enumerate(X):\n",
    "        \n",
    "        y_pred = np.dot(sample, W) + bias\n",
    "        loss = ((y_pred - y[sample_idx]) ** 2) / 2\n",
    "        \n",
    "        W = W - learning_rate * (y_pred - y[sample_idx]) * sample\n",
    "        bias = bias - learning_rate * (y_pred - y[sample_idx])\n",
    "        \n",
    "    acc = (np.sign(np.dot(X, W) + bias) == y).mean()\n",
    "    print(acc)\n",
    "        \n",
    "        # plot_decision_boundary(X, y, W, bias, sample, y[sample_idx])\n",
    "        \n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T11:33:24.611818700Z",
     "start_time": "2024-10-24T11:33:24.593819300Z"
    }
   },
   "id": "6e3351663f3bae7b"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.75\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.75\n",
      "0.25\n",
      "0.75\n",
      "0.75\n",
      "0.25\n",
      "0.5\n",
      "0.25\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.25\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # datele XOR\n",
    "y = np.array( [-1, 1, 1, -1]) # etichetele\n",
    "W = np.zeros(X.shape[1]) # ponderi\n",
    "bias = 0\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 0.1\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    for sample_idx, sample in enumerate(X):\n",
    "        \n",
    "        y_pred = np.dot(sample, W) + bias\n",
    "        loss = ((y_pred - y[sample_idx]) ** 2) / 2\n",
    "        \n",
    "        W = W - learning_rate * (y_pred - y[sample_idx]) * sample\n",
    "        bias = bias - learning_rate * (y_pred - y[sample_idx])\n",
    "        \n",
    "    acc = (np.sign(np.dot(X, W) + bias) == y).mean()\n",
    "    print(acc)\n",
    "        \n",
    "        # plot_decision_boundary(X, y, W, bias, sample, y[sample_idx])\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T11:33:32.745602700Z",
     "start_time": "2024-10-24T11:33:32.703466600Z"
    }
   },
   "id": "39076654fb5addc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rețea neuronală pentru XOR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d52ca46fb214a0"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    " return 1/(1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T12:19:18.691909500Z",
     "start_time": "2024-10-24T12:19:18.675882800Z"
    }
   },
   "id": "d84ce8adbf4d35e6"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def compute_y(x, W, bias):\n",
    "    return (-x*W[0] - bias) / (W[1] + 1e-10)\n",
    "\n",
    "def plot_decision(X_, W_1, W_2, b_1, b_2):\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.ylim((-0.5, 1.5))\n",
    "    plt.xlim((-0.5, 1.5))\n",
    "    xx = np.random.normal(0, 1, (100000))\n",
    "    yy = np.random.normal(0, 1, (100000))\n",
    "    X = np.array([xx, yy]).transpose()\n",
    "    X = np.concatenate((X, X_)) \n",
    "    _, _, _, output = forward(X, W_1, b_1, W_2, b_2)\n",
    "    y = np.squeeze(np.round(output))\n",
    "    plt.plot(X[y == 0, 0], X[y == 0, 1], 'b+')\n",
    "    plt.plot(X[y == 1, 0], X[y == 1, 1], 'r+')\n",
    "    plt.show(block=False) \n",
    "    plt.pause(0.1)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T12:19:19.998947500Z",
     "start_time": "2024-10-24T12:19:19.986907300Z"
    }
   },
   "id": "9ba5cc169ec61ffc"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def forward(X, W_1, b_1, W_2, b_2):\n",
    "    \n",
    "    # shape X =  (4, 2)\n",
    "    # shape W1 = (2, 5)\n",
    "    # shape b1 = (1, 5)\n",
    "    # shape W2 = (5, 1)\n",
    "    # shape b2 = (1)\n",
    "    # shape Y  = (4, 1)\n",
    "    \n",
    "    z_1 = np.dot(X, W_1) + b_1   # shape z_1 = (4, 5)\n",
    "    a_1 = np.tanh(z_1)           # shape a_1 = (4, 5)\n",
    "    z_2 = np.dot(a_1, W_2) + b_2 # shape z_2 = (4, 1)\n",
    "    a_2 = sigmoid(z_2)           # shape a_2 = (4, 1)\n",
    "    \n",
    "    return z_1, a_1, z_2, a_2 "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T13:02:31.788814400Z",
     "start_time": "2024-10-24T13:02:31.776788100Z"
    }
   },
   "id": "d110b8a3547780ef"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "def backward(a_1, a_2, z_1, W_2, X, Y, num_samples):\n",
    "    \n",
    "    dz_2 = a_2.flatten() - Y.flatten()                    # shape dz_2 = (4, 1)\n",
    "    dw_2 = np.dot(np.transpose(a_1), dz_2) / num_samples  # shape dw_1 = (5, 1)\n",
    "    db_2 = sum(dz_2) / num_samples                        # shape db_2 = (1)\n",
    "    da_1 = np.dot(dz_2.reshape((4,1)), np.transpose(W_2).reshape((1,5))) # shape da_1 = (4, 5) \n",
    "    \n",
    "    dz_1 = da_1 * (1 - np.tanh(z_1) ** 2)\n",
    "    dw_1 = np.dot(np.transpose(X), dz_1) / num_samples\n",
    "    db_1 = np.sum(dz_1) / num_samples\n",
    "    \n",
    "    return dw_1, db_1, dw_2, db_2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T14:03:20.241664600Z",
     "start_time": "2024-10-24T14:03:20.233148300Z"
    }
   },
   "id": "2a2ccfd6f576c2d5"
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: acc = 0.5 [1. 0. 1. 0.] loss= 0.768175101203795\n",
      "epoch 2: acc = 0.75 [0. 0. 0. 1.] loss= 0.7192859014803898\n",
      "epoch 3: acc = 0.5 [0. 0. 1. 1.] loss= 0.6782017588597264\n",
      "epoch 4: acc = 0.75 [1. 0. 0. 0.] loss= 0.6535311489755493\n",
      "epoch 5: acc = 0.75 [1. 1. 0. 1.] loss= 0.6317233234653541\n",
      "epoch 6: acc = 0.75 [1. 0. 0. 0.] loss= 0.6154703547447925\n",
      "epoch 7: acc = 0.75 [1. 0. 1. 1.] loss= 0.6003063246604492\n",
      "epoch 8: acc = 0.75 [0. 0. 1. 0.] loss= 0.5874605523950427\n",
      "epoch 9: acc = 0.75 [1. 1. 0. 1.] loss= 0.5749797050352424\n",
      "epoch 10: acc = 0.75 [0. 1. 0. 0.] loss= 0.5635182362290463\n",
      "epoch 11: acc = 1.0 [0. 0. 1. 1.] loss= 0.5521050410249156\n",
      "epoch 12: acc = 1.0 [0. 1. 0. 1.] loss= 0.541121057278557\n",
      "epoch 13: acc = 1.0 [0. 1. 0. 1.] loss= 0.5300473534309884\n",
      "epoch 14: acc = 1.0 [1. 0. 0. 1.] loss= 0.5191284800891505\n",
      "epoch 15: acc = 1.0 [1. 0. 0. 1.] loss= 0.5080752207376642\n",
      "epoch 16: acc = 1.0 [1. 1. 0. 0.] loss= 0.4970612950778501\n",
      "epoch 17: acc = 1.0 [1. 1. 0. 0.] loss= 0.4859231678922662\n",
      "epoch 18: acc = 1.0 [0. 1. 1. 0.] loss= 0.47479383149111126\n",
      "epoch 19: acc = 1.0 [1. 0. 0. 1.] loss= 0.4635828885338331\n",
      "epoch 20: acc = 1.0 [1. 0. 1. 0.] loss= 0.4523956330963905\n",
      "epoch 21: acc = 1.0 [1. 0. 0. 1.] loss= 0.44118636041078474\n",
      "epoch 22: acc = 1.0 [1. 0. 1. 0.] loss= 0.430038034745451\n",
      "epoch 23: acc = 1.0 [1. 0. 1. 0.] loss= 0.418932855378203\n",
      "epoch 24: acc = 1.0 [0. 1. 1. 0.] loss= 0.40793395130297516\n",
      "epoch 25: acc = 1.0 [1. 0. 1. 0.] loss= 0.39704025865371767\n",
      "epoch 26: acc = 1.0 [1. 1. 0. 0.] loss= 0.3862971450106999\n",
      "epoch 27: acc = 1.0 [0. 0. 1. 1.] loss= 0.3757120320017569\n",
      "epoch 28: acc = 1.0 [0. 0. 1. 1.] loss= 0.36531527236191474\n",
      "epoch 29: acc = 1.0 [0. 1. 0. 1.] loss= 0.35511675256671815\n",
      "epoch 30: acc = 1.0 [1. 0. 1. 0.] loss= 0.34513530303362383\n",
      "epoch 31: acc = 1.0 [0. 1. 0. 1.] loss= 0.3353796343416719\n",
      "epoch 32: acc = 1.0 [0. 0. 1. 1.] loss= 0.3258606286299228\n",
      "epoch 33: acc = 1.0 [1. 0. 1. 0.] loss= 0.3165843599164981\n",
      "epoch 34: acc = 1.0 [1. 0. 1. 0.] loss= 0.3075565769729054\n",
      "epoch 35: acc = 1.0 [0. 1. 1. 0.] loss= 0.29878056228022004\n",
      "epoch 36: acc = 1.0 [0. 1. 1. 0.] loss= 0.2902586198321642\n",
      "epoch 37: acc = 1.0 [1. 0. 0. 1.] loss= 0.28199150224084174\n",
      "epoch 38: acc = 1.0 [1. 1. 0. 0.] loss= 0.27397891302730537\n",
      "epoch 39: acc = 1.0 [1. 1. 0. 0.] loss= 0.2662193991284714\n",
      "epoch 40: acc = 1.0 [1. 0. 0. 1.] loss= 0.25871057273962894\n",
      "epoch 41: acc = 1.0 [1. 0. 1. 0.] loss= 0.25144915930370015\n",
      "epoch 42: acc = 1.0 [1. 0. 0. 1.] loss= 0.2444311444743384\n",
      "epoch 43: acc = 1.0 [0. 1. 1. 0.] loss= 0.23765187010518057\n",
      "epoch 44: acc = 1.0 [0. 1. 1. 0.] loss= 0.23110615054715666\n",
      "epoch 45: acc = 1.0 [0. 0. 1. 1.] loss= 0.22478836911314581\n",
      "epoch 46: acc = 1.0 [0. 1. 1. 0.] loss= 0.21869257085853414\n",
      "epoch 47: acc = 1.0 [1. 0. 0. 1.] loss= 0.21281254341009592\n",
      "epoch 48: acc = 1.0 [1. 0. 0. 1.] loss= 0.20714188935198186\n",
      "epoch 49: acc = 1.0 [0. 1. 0. 1.] loss= 0.20167408942363663\n",
      "epoch 50: acc = 1.0 [0. 1. 0. 1.] loss= 0.19640255764959355\n",
      "epoch 51: acc = 1.0 [0. 1. 0. 1.] loss= 0.19132068894870286\n",
      "epoch 52: acc = 1.0 [0. 1. 0. 1.] loss= 0.18642189995193137\n",
      "epoch 53: acc = 1.0 [1. 0. 0. 1.] loss= 0.18169966368100973\n",
      "epoch 54: acc = 1.0 [1. 0. 0. 1.] loss= 0.17714753871651084\n",
      "epoch 55: acc = 1.0 [1. 0. 0. 1.] loss= 0.17275919344471008\n",
      "epoch 56: acc = 1.0 [1. 0. 0. 1.] loss= 0.16852842593158476\n",
      "epoch 57: acc = 1.0 [1. 0. 1. 0.] loss= 0.16444917993153962\n",
      "epoch 58: acc = 1.0 [0. 1. 1. 0.] loss= 0.1605155574971943\n",
      "epoch 59: acc = 1.0 [1. 0. 1. 0.] loss= 0.15672182861639813\n",
      "epoch 60: acc = 1.0 [1. 0. 1. 0.] loss= 0.15306243826371382\n",
      "epoch 61: acc = 1.0 [1. 0. 1. 0.] loss= 0.1495320112164379\n",
      "epoch 62: acc = 1.0 [1. 0. 1. 0.] loss= 0.146125354950033\n",
      "epoch 63: acc = 1.0 [0. 1. 1. 0.] loss= 0.14283746089483562\n",
      "epoch 64: acc = 1.0 [1. 1. 0. 0.] loss= 0.13966350430516244\n",
      "epoch 65: acc = 1.0 [0. 1. 1. 0.] loss= 0.13659884296353014\n",
      "epoch 66: acc = 1.0 [1. 1. 0. 0.] loss= 0.13363901491660884\n",
      "epoch 67: acc = 1.0 [0. 0. 1. 1.] loss= 0.1307797354157076\n",
      "epoch 68: acc = 1.0 [0. 1. 1. 0.] loss= 0.12801689321296855\n",
      "epoch 69: acc = 1.0 [0. 0. 1. 1.] loss= 0.12534654634490944\n",
      "epoch 70: acc = 1.0 [0. 1. 0. 1.] loss= 0.12276491751740798\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # datele XOR\n",
    "Y = np.array( [0, 1, 1, 0]) # etichetele\n",
    "W = np.zeros(X.shape[1]) # ponderi\n",
    "bias = 0\n",
    "\n",
    "num_epochs = 70\n",
    "learning_rate = 0.5\n",
    "num_hidden_neurons = 5\n",
    "miu = 0 # medie\n",
    "sigma = 1 # deviatia standard\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "W_1 = rng.normal(loc=miu, scale=sigma, size=(X.shape[1], num_hidden_neurons))\n",
    "b_1 = np.zeros(num_hidden_neurons)\n",
    "\n",
    "W_2 = rng.normal(loc=miu, scale=sigma, size=num_hidden_neurons)\n",
    "b_2 = np.zeros(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    X, Y = shuffle(X, Y)\n",
    "    \n",
    "    z_1, a_1, z_2, a_2 = forward(X, W_1, b_1, W_2, b_2)\n",
    "    loss = (-Y * np.log(a_2) - (1 - Y) * np.log(1 - a_2)).mean()\n",
    "    accuracy = (np.round(a_2) == Y).mean()\n",
    "    \n",
    "    print(f'epoch {epoch+1}: acc = ', end='')\n",
    "    print(accuracy, end=' ')\n",
    "    print(np.round(a_2), end=' ')\n",
    "    print(f'loss= {loss}')\n",
    "    \n",
    "    dw_1, db_1, dw_2, db_2 = backward(a_1, a_2, z_1, W_2, X, Y, len(X))\n",
    "    \n",
    "    W_1 -= learning_rate * dw_1 \n",
    "    b_1 -= learning_rate * db_1\n",
    "    W_2 -= learning_rate * dw_2\n",
    "    b_2 -= learning_rate * db_2\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T14:08:07.280233300Z",
     "start_time": "2024-10-24T14:08:07.213522800Z"
    }
   },
   "id": "278e83497c4e24d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
